# File: chay_tren_pi.py
import cv2
import numpy as np
import time
# Tr√™n Pi 3 kh√¥ng c√†i tensorflow, ch·ªâ c√†i tflite_runtime cho nh·∫π
try:
    import tflite_runtime.interpreter as tflite
except ImportError:
    # Ph√≤ng tr∆∞·ªùng h·ª£p b·∫°n test tr√™n Laptop th√¨ v·∫´n ch·∫°y ƒë∆∞·ª£c b·∫±ng tensorflow
    import tensorflow.lite as tflite

# --- C·∫§U H√åNH ---
MODEL_PATH = "saurang_pi_final.tflite" # T√™n file model b·∫°n v·ª´a t·∫£i v·ªÅ
IMG_SIZE = 224      # K√≠ch th∆∞·ªõc ·∫£nh l√∫c train (B·∫ÆT BU·ªòC KH·ªöP)
CONFIDENCE = 0.5    # ƒê·ªô nh·∫°y (0.5 l√† trung b√¨nh)

def main():
    print(f"üîÑ Dang load model: {MODEL_PATH}...")
    try:
        interpreter = tflite.Interpreter(model_path=MODEL_PATH)
        interpreter.allocate_tensors()
    except Exception as e:
        print("‚ùå L·ªñI: Kh√¥ng t√¨m th·∫•y file model. H√£y ch·∫Øc ch·∫Øn file .tflite n·∫±m chung th∆∞ m·ª•c!")
        return

    # L·∫•y th√¥ng tin Input/Output c·ªßa model
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()
    
    print("üì∑ Dang khoi dong Camera...")
    cap = cv2.VideoCapture(0) # S·ªë 0 l√† camera m·∫∑c ƒë·ªãnh
    
    # C√†i ƒë·∫∑t k√≠ch th∆∞·ªõc khung h√¨nh camera th·∫•p xu·ªëng ƒë·ªÉ gi·∫£m t·∫£i cho Pi 3
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)

    prev_frame_time = 0
    new_frame_time = 0

    print("‚úÖ Bat dau soi rang! Nhan 'q' de thoat.")

    while True:
        ret, frame = cap.read()
        if not ret:
            print("‚ùå L·ªói Camera!")
            break

        # 1. PRE-PROCESSING (X·ª≠ l√Ω ·∫£nh tr∆∞·ªõc khi ƒë∆∞a v√†o AI)
        # Resize v·ªÅ 224x224
        img_resized = cv2.resize(frame, (IMG_SIZE, IMG_SIZE))
        # Chu·∫©n h√≥a m√†u s·∫Øc (chia 255) v√† ƒë·ªïi sang float32
        input_data = np.expand_dims(img_resized, axis=0).astype(np.float32) / 255.0

        # 2. RUN MODEL (Ch·∫°y AI)
        interpreter.set_tensor(input_details[0]['index'], input_data)
        
        start_time = time.time()
        interpreter.invoke() # ƒê√¢y l√† l·ªánh b·∫Øt AI suy nghƒ©
        
        # 3. POST-PROCESSING (X·ª≠ l√Ω k·∫øt qu·∫£ ƒë·∫ßu ra)
        # K·∫øt qu·∫£ l√† m·ªôt c√°i ·∫£nh Mask (ƒëen tr·∫Øng) b·ªã n√©n nh·ªè
        output_data = interpreter.get_tensor(output_details[0]['index'])[0]
        
        # T√≠nh FPS
        new_frame_time = time.time()
        fps = 1 / (new_frame_time - prev_frame_time)
        prev_frame_time = new_frame_time

        # X·ª≠ l√Ω mask: Ch·ªó n√†o > 0.5 th√¨ cho l√† s√¢u rƒÉng
        mask = (output_data > CONFIDENCE).astype(np.uint8) * 255
        
        # Resize mask to b·∫±ng k√≠ch th∆∞·ªõc khung h√¨nh camera th·∫≠t ƒë·ªÉ v·∫Ω ƒë√® l√™n
        mask_overlay = cv2.resize(mask, (frame.shape[1], frame.shape[0]), interpolation=cv2.INTER_NEAREST)
        
        # --- T·∫†O HI·ªÜU ·ª®NG T√î M√ÄU ---
        # T√¨m c√°c ƒë∆∞·ªùng vi·ªÅn c·ªßa v√πng s√¢u rƒÉng ƒë·ªÉ v·∫Ω cho ƒë·∫πp
        contours, _ = cv2.findContours(mask_overlay, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        # V·∫Ω vi·ªÅn m√†u ƒê·ªè (BGR: 0, 0, 255) l√™n ·∫£nh g·ªëc
        cv2.drawContours(frame, contours, -1, (0, 0, 255), 2)
        
        # T√¥ m√†u b√°n trong su·ªët (Overlay)
        # T·∫°o m·ªôt l·ªõp m√†u ƒë·ªè
        colored_layer = np.zeros_like(frame)
        colored_layer[:, :, 2] = mask_overlay # K√™nh ƒë·ªè
        
        # Tr·ªôn ·∫£nh g·ªëc v√† l·ªõp m√†u ƒë·ªè
        frame = cv2.addWeighted(frame, 1.0, colored_layer, 0.4, 0) # 0.4 l√† ƒë·ªô ƒë·∫≠m

        # Hi·ªán FPS l√™n m√†n h√¨nh
        cv2.putText(frame, f"FPS: {int(fps)}", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
        cv2.putText(frame, "SO DUNG CUA BAN - BAM 'Q' DE THOAT", (10, frame.shape[0] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)

        cv2.imshow("Phat Hien Sau Rang (Pi 3B+)", frame)

        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

    cap.release()
    cv2.destroyAllWindows()

if __name__ == "__main__":
    main()
